{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center><font color='black'> DEEPTHI SUDHARSAN (ROLL NUMBER : CB.EN.U4AIE19022)</font></center>\n",
    "## <center><font color='black'> AI IN NLP ASSIGNMENT 6 </font></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Explain about vanishing gradient problem RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The gradients carry information used during the parameter updates (here, the RNN parameter updates) during backpropagation. During the update, the gradient becomes smaller and smaller (because we are going on multiplying, if the values are already small, and we multiply them, it becomes smaller) to the point that the parameter updates become insignificant so no learning is done. This is called the vanishing gradient problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### 2. Explain about gradient explosion problem in RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploding gradient is the inverse of vanishing gradient. Instead of the parameter updates becoming insignificant like in the case of vanishing gradient problem, here, the RNN parameter updates accumulate during backpropagation and result in extremely large update values for the model weights that are sometimes even difficult to store and remember because of which, the model is unstable and incapable of learning from the data and forgets the update values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### 3. What are the different ways to overcome these issues ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - In the case of the vanishing gradient problem, we can use non saturating activation functions such as ReLU etc., which does not lead to smaller values (thereby preventing from the values becoming insignificant). Residual networks (skip connections) can also help overcome vanishing gradient problem. With respect to the issue in RNN, we can go for LSTM (Long short term memory) to mitigate this problem.\n",
    "\n",
    "### - In the case of exploding gradient problem, the issue arises due to large accumulation of parameter updates. So, to mitigate the issue, we do gradient clipping, i.e., we clip the gradients during backpropagation so that the update values due to cross a certain threshold (that has been defined for the clipping process).\n",
    "\n",
    "### Proper weight initialization and batch normalization can also help with overcoming vanishing gradient and exploding gradient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** \n",
    "### 4.\tImplement a forward and backward propagation of simple neural network using ‘numpy’ library. The network has the following description, Input layer (3 neurons), hidden layer (4 neurons), output layer (2 neurons)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT VECTOR : \n",
      " [[5 2 2]]\n",
      "OUTPUT : \n",
      " [[0.02348475 0.03905022]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# creating a neural network class to create a neural network with an input layer and a hidden layer and an output layer\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, input_size = 3, hidden_size = 4, output_size = 2):\n",
    "        # initializing the size (number of neurons) of the input, hidden and output layers\n",
    "        # In our question it is given as Input layer (3 neurons), hidden layer (4 neurons), output layer (2 neurons)\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        # initializing the weights matrix randomly for the neurons in the hidden and output layers\n",
    "        self.W1 = np.random.randn(self.input_size, self.hidden_size)\n",
    "        self.W2 = np.random.randn(self.hidden_size, self.output_size)\n",
    "        # initializing the bias vector randomly for the neurons in the hidden and output layers\n",
    "        self.b1 = np.zeros((1, self.hidden_size))\n",
    "        self.b2 = np.zeros((1, self.output_size))\n",
    "\n",
    "    # forward propagation\n",
    "    def forward(self, X):\n",
    "#         print(\"FORWARD PROPAGATION IS BEING DONE .......\")\n",
    "        # forward propagation Z and A update\n",
    "        # Formula: Z_n = A_n-1 . W_n + b_n\n",
    "        # A_n = ActivationFn(Z_n)\n",
    "        # where W is weight matrix and b is bias vector\n",
    "        # for the first hidden layer, we will do dot product on input layer and the weights of the hidden layer\n",
    "        # and add bias vector of the hidden layer to it\n",
    "        self.z2 = np.dot(X, self.W1) + self.b1\n",
    "        # after passing output throught activation function, here, sigmoid we get A2\n",
    "        self.a2 = self.sigmoid(self.z2)\n",
    "        # for the output layer we do dot product of the output from the activation function in the previous (hidden)\n",
    "        # layer along with the weights of the neurons in the output layer\n",
    "        # and add bias vector of the output layer to it\n",
    "        self.z3 = np.dot(self.a2, self.W2) + self.b2\n",
    "        # after passing output throught activation function, here, sigmoid we get our NN's output (in this given case of\n",
    "        # neural network with 2 layers (including the output layer). \n",
    "        y_hat = self.sigmoid(self.z3)\n",
    "        return y_hat\n",
    "\n",
    "    # sigmoid activation function\n",
    "    def sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "    # derivative of the simoid activation function (for backpropagation parameter update)\n",
    "    def sigmoid_prime(self, z):\n",
    "        return self.sigmoid(z) * (1 - self.sigmoid(z))\n",
    "    \n",
    "    # backpropagation\n",
    "    def backward(self, X, y, y_hat):\n",
    "#         print(\"BACKPROPAGATION IS BEING DONE .......\")\n",
    "        # y-y_hat by doing so we calculate the slope of the linear step we take\n",
    "        delta3 = np.multiply(-(y - y_hat), self.sigmoid_prime(self.z3))\n",
    "        # we calculate the slope of the loss function with respect to our weights and biases propagating backwards \n",
    "        # from the output to the input layer.\n",
    "        dJdW2 = np.dot(self.a2.T, delta3)\n",
    "        delta2 = np.dot(delta3, self.W2.T) * self.sigmoid_prime(self.z2)\n",
    "        dJdW1 = np.dot(X.T, delta2)\n",
    "        # calculating the new updated bias vectors for both the layers\n",
    "        dJdb1 = np.sum(delta2, axis=0, keepdims=True)\n",
    "        dJdb2 = np.sum(delta3, axis=0, keepdims=True)\n",
    "        return dJdW1, dJdW2, dJdb1, dJdb2\n",
    "\n",
    "# class to train the neural netwotk\n",
    "class Trainer:\n",
    "    def __init__(self, N):\n",
    "        # initializing the neural network class object to N \n",
    "        self.N = N\n",
    "\n",
    "    def train(self, X, y):\n",
    "        # calling the forward propgation function\n",
    "        self.N.forward(X)\n",
    "        # alpha or learning rate\n",
    "        learning_rate_alpha = 0.01\n",
    "        # calling the backpropagation function and calculating the gradients\n",
    "        dJdW1, dJdW2, dJdb1, dJdb2 = self.N.backward(X, y, self.N.forward(X))\n",
    "        # updating the neural network's weight matrix and bias vectors of the layers\n",
    "        self.N.W1 -= learning_rate_alpha * dJdW1\n",
    "        self.N.W2 -= learning_rate_alpha * dJdW2\n",
    "        self.N.b1 -= learning_rate_alpha * dJdb1\n",
    "        self.N.b2 -= learning_rate_alpha * dJdb2\n",
    "\n",
    "# Input vector with 3 neurons\n",
    "input_vector = np.random.randint(0,9,(1,3))\n",
    "print(\"INPUT VECTOR : \\n\",input_vector)\n",
    "y = np.random.randint(0,5)\n",
    "N = NeuralNetwork() # default network Input layer (3 neurons), hidden layer (4 neurons), output layer (2 neurons)\n",
    "T = Trainer(N) # calling the training class constructor with the neural network class object as parameter\n",
    "# number of iterations for us to train (forward and bacpropagate) and get the output\n",
    "niters = 10000\n",
    "for i in range(niters):\n",
    "    # calling the train function defined in the trainer class where forward and backpropagation call and update is done\n",
    "    T.train(input_vector, y)\n",
    "# printing the output from the forward propagation after training for 10000 iterations\n",
    "print(\"OUTPUT : \\n\",N.forward(input_vector))\n",
    "# print(N.forward(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
