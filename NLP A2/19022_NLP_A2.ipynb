{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center><font color='black'> DEEPTHI SUDHARSAN (ROLL NUMBER : CB.EN.U4AIE19022)</font></center>\n",
    "## <center><font color='black'> AI IN NLP ASSIGNMENT 2 </font></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activity\n",
      "\n",
      "Use the below text content, write python code for the following task\n",
      "1. Convert the paragraph into sentences\n",
      "2. Append the start and end tags\n",
      "3. Display bigrams, trigrams, 4-grams\n",
      "4. Display the probability for bigrams, trigrams, 4-grams\n",
      "\n",
      "Note:\n",
      "Use ‘empty space’ as the delimiter for tokenization\n",
      "The below text content is taken from https://ai.googleblog.com/2006/08/all-our-n-gram-are-belong-to-you.html\n",
      "\n",
      "Content:\n",
      "Here at Google Research we have been using word  for a variety of R&D projects, such as , speech recognition, , entity detection, information extraction, and others. While such models have usually been estimated from training corpora containing at most a few billion words, we have been harnessing the vast power of Google's datacenters and distributed processing  to process larger and larger training corpora. We found that there's no data like more data, and scaled up the size of our data by one order of magnitude, and then another, and then one more - resulting in a training corpus of one trillion words from public Web pages. We believe that the entire research community can benefit from access to such massive amounts of data. It will advance the state of the art, it will focus research in the promising direction of large-scale, data-driven approaches, and it will allow all research groups, no matter how large or small their computing resources, to play together. That's why we decided to share this enormous dataset with everyone. We processed 1,024,908,267,229 words of running text and are publishing the counts for all 1,176,470,663 five-word sequences that appear at least 40 times. There are 13,588,391 unique words, after discarding words that appear less than 200 times.\n"
     ]
    }
   ],
   "source": [
    "# !pip install python-docx\n",
    "import docx\n",
    "from docx import Document\n",
    "\n",
    "document = Document('Activity 2.docx')\n",
    "for para in document.paragraphs:\n",
    "    print(para.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Here at Google Research we have been using word n-gram models for a variety of R&D projects, such as statistical machine translation, speech recognition, spelling correction, entity detection, information extraction, and others. While such models have usually been estimated from training corpora containing at most a few billion words, we have been harnessing the vast power of Google's datacenters and distributed processing infrastructure to process larger and larger training corpora. We found that there's no data like more data, and scaled up the size of our data by one order of magnitude, and then another, and then one more - resulting in a training corpus of one trillion words from public Web pages. We believe that the entire research community can benefit from access to such massive amounts of data. It will advance the state of the art, it will focus research in the promising direction of large-scale, data-driven approaches, and it will allow all research groups, no matter how large or small their computing resources, to play together. That's why we decided to share this enormous dataset with everyone. We processed 1,024,908,267,229 words of running text and are publishing the counts for all 1,176,470,663 five-word sequences that appear at least 40 times. There are 13,588,391 unique words, after discarding words that appear less than 200 times.\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"Here at Google Research we have been using word n-gram models for a variety of R&D projects, such as statistical machine translation, speech recognition, spelling correction, entity detection, information extraction, and others. While such models have usually been estimated from training corpora containing at most a few billion words, we have been harnessing the vast power of Google's datacenters and distributed processing infrastructure to process larger and larger training corpora. We found that there's no data like more data, and scaled up the size of our data by one order of magnitude, and then another, and then one more - resulting in a training corpus of one trillion words from public Web pages. We believe that the entire research community can benefit from access to such massive amounts of data. It will advance the state of the art, it will focus research in the promising direction of large-scale, data-driven approaches, and it will allow all research groups, no matter how large or small their computing resources, to play together. That's why we decided to share this enormous dataset with everyone. We processed 1,024,908,267,229 words of running text and are publishing the counts for all 1,176,470,663 five-word sequences that appear at least 40 times. There are 13,588,391 unique words, after discarding words that appear less than 200 times.\"\n",
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert the paragraph to sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Here at Google Research we have been using word n-gram models for a variety of R&D projects, such as statistical machine translation, speech recognition, spelling correction, entity detection, information extraction, and others',\n",
       " \"While such models have usually been estimated from training corpora containing at most a few billion words, we have been harnessing the vast power of Google's datacenters and distributed processing infrastructure to process larger and larger training corpora\",\n",
       " \"We found that there's no data like more data, and scaled up the size of our data by one order of magnitude, and then another, and then one more - resulting in a training corpus of one trillion words from public Web pages\",\n",
       " 'We believe that the entire research community can benefit from access to such massive amounts of data',\n",
       " 'It will advance the state of the art, it will focus research in the promising direction of large-scale, data-driven approaches, and it will allow all research groups, no matter how large or small their computing resources, to play together',\n",
       " \"That's why we decided to share this enormous dataset with everyone\",\n",
       " 'We processed 1,024,908,267,229 words of running text and are publishing the counts for all 1,176,470,663 five-word sequences that appear at least 40 times',\n",
       " 'There are 13,588,391 unique words, after discarding words that appear less than 200 times.']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = text.split('. ')\n",
    "# sentences.pop() \n",
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Here at Google Research we have been using word n gram models for a variety of R D projects  such as statistical machine translation  speech recognition  spelling correction  entity detection  information extraction  and others',\n",
       " 'While such models have usually been estimated from training corpora containing at most a few billion words  we have been harnessing the vast power of Google s datacenters and distributed processing infrastructure to process larger and larger training corpora',\n",
       " 'We found that there s no data like more data  and scaled up the size of our data by one order of magnitude  and then another  and then one more   resulting in a training corpus of one trillion words from public Web pages',\n",
       " 'We believe that the entire research community can benefit from access to such massive amounts of data',\n",
       " 'It will advance the state of the art  it will focus research in the promising direction of large scale  data driven approaches  and it will allow all research groups  no matter how large or small their computing resources  to play together',\n",
       " 'That s why we decided to share this enormous dataset with everyone',\n",
       " 'We processed                   words of running text and are publishing the counts for all               five word sequences that appear at least    times',\n",
       " 'There are            unique words  after discarding words that appear less than     times ']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# bit of preprocessing\n",
    "import re\n",
    "for i in range(len(sentences)):\n",
    "    sentences[i] = re.sub('[^a-zA-Z\\s]', ' ', sentences[i])\n",
    "sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Append the start and end tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s> Here at Google Research we have been using word n gram models for a variety of R D projects  such as statistical machine translation  speech recognition  spelling correction  entity detection  information extraction  and others </s>'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blines = [\"<s> \" + s + \" </s>\" for s in sentences]\n",
    "blines[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s1> <s> Here at Google Research we have been using word n gram models for a variety of R D projects  such as statistical machine translation  speech recognition  spelling correction  entity detection  information extraction  and others </s> </s1>'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tlines = [\"<s1> \" + s + \" </s1>\" for s in blines]\n",
    "tlines[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s2> <s1> <s> Here at Google Research we have been using word n gram models for a variety of R D projects  such as statistical machine translation  speech recognition  spelling correction  entity detection  information extraction  and others </s> </s1> </s2>'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flines = [\"<s2> \" + s + \" </s2>\" for s in tlines]\n",
    "flines[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display bigrams, trigrams, 4-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter, defaultdict\n",
    "def N_gram(words,n):\n",
    "    ngrams_list = []\n",
    "    for i in range(len(words)-n+1):\n",
    "        temp_list = [words[j] for j in range(i,i+n)]\n",
    "        ngrams_list.append(\" \".join(temp_list))\n",
    "    return ngrams_list\n",
    "\n",
    "def word_list(lines):\n",
    "    words = []\n",
    "    for i in range(len(lines)):\n",
    "        words.extend(lines[i].split())\n",
    "    return words\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bigrams\n",
    "bigrams = N_gram(word_list(blines),2)\n",
    "# trigrams\n",
    "trigrams = N_gram(word_list(tlines),3)\n",
    "# 4-grams \n",
    "fourgrams = N_gram(word_list(flines),4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first bigram, trigram and 4-gram  are :  <s> Here   <s1> <s> Here   <s2> <s1> <s> Here\n"
     ]
    }
   ],
   "source": [
    "print(\"The first bigram, trigram and 4-gram  are : \",bigrams[0],\" \",trigrams[0],\" \",fourgrams[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display the probability for bigrams, trigrams, 4-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngram_probability(sentence,n):\n",
    "    # to store the probabilities\n",
    "    probs = defaultdict(list)\n",
    "    ngrams = []\n",
    "    prev = []\n",
    "    for s in sentence:\n",
    "        # split sentence to words\n",
    "        w = s.split()\n",
    "        # getting the n grams and n-1 grams\n",
    "        ngrams.extend(N_gram(w,n))\n",
    "        prev.extend(N_gram(w,n-1))\n",
    "    words = ' '.join(sentence).split()\n",
    "    nlength = len(N_gram(words,n))\n",
    "    # Term frequencies of the n grams and n-1 grams\n",
    "    ngramsTF = dict(Counter(ngrams))\n",
    "    prevTF = dict(Counter(prev))\n",
    "    # store the key\n",
    "    probskey = \"\"\n",
    "    # loop through and calculate probability and add it to dict\n",
    "    for i in range(nlength):\n",
    "        j = i + n - 1\n",
    "        prevwords =  ' '.join(words[j-n+1:j])\n",
    "        curr = words[j]\n",
    "        probskey = \"P(\" + curr + \"|\" + prevwords + \")\"\n",
    "        val = prevwords + \" \" + curr\n",
    "        if prevTF.get(prevwords) != None:\n",
    "            probs[probskey] = ngramsTF.get(val,0.0) / prevTF.get(prevwords)\n",
    "        else:\n",
    "            probs[probskey] = 0.0\n",
    "    return dict(probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'P(Here|<s>)': 0.125,\n",
       " 'P(at|Here)': 1.0,\n",
       " 'P(Google|at)': 0.3333333333333333,\n",
       " 'P(Research|Google)': 0.5,\n",
       " 'P(we|Research)': 1.0,\n",
       " 'P(have|we)': 0.6666666666666666,\n",
       " 'P(been|have)': 0.6666666666666666,\n",
       " 'P(using|been)': 0.3333333333333333,\n",
       " 'P(word|using)': 1.0,\n",
       " 'P(n|word)': 0.5,\n",
       " 'P(gram|n)': 1.0,\n",
       " 'P(models|gram)': 1.0,\n",
       " 'P(for|models)': 0.5,\n",
       " 'P(a|for)': 0.5,\n",
       " 'P(variety|a)': 0.3333333333333333,\n",
       " 'P(of|variety)': 1.0,\n",
       " 'P(R|of)': 0.1111111111111111,\n",
       " 'P(D|R)': 1.0,\n",
       " 'P(projects|D)': 1.0,\n",
       " 'P(such|projects)': 1.0,\n",
       " 'P(as|such)': 0.3333333333333333,\n",
       " 'P(statistical|as)': 1.0,\n",
       " 'P(machine|statistical)': 1.0,\n",
       " 'P(translation|machine)': 1.0,\n",
       " 'P(speech|translation)': 1.0,\n",
       " 'P(recognition|speech)': 1.0,\n",
       " 'P(spelling|recognition)': 1.0,\n",
       " 'P(correction|spelling)': 1.0,\n",
       " 'P(entity|correction)': 1.0,\n",
       " 'P(detection|entity)': 1.0,\n",
       " 'P(information|detection)': 1.0,\n",
       " 'P(extraction|information)': 1.0,\n",
       " 'P(and|extraction)': 1.0,\n",
       " 'P(others|and)': 0.125,\n",
       " 'P(</s>|others)': 1.0,\n",
       " 'P(<s>|</s>)': 0.0,\n",
       " 'P(While|<s>)': 0.125,\n",
       " 'P(such|While)': 1.0,\n",
       " 'P(models|such)': 0.3333333333333333,\n",
       " 'P(have|models)': 0.5,\n",
       " 'P(usually|have)': 0.3333333333333333,\n",
       " 'P(been|usually)': 1.0,\n",
       " 'P(estimated|been)': 0.3333333333333333,\n",
       " 'P(from|estimated)': 1.0,\n",
       " 'P(training|from)': 0.3333333333333333,\n",
       " 'P(corpora|training)': 0.6666666666666666,\n",
       " 'P(containing|corpora)': 0.5,\n",
       " 'P(at|containing)': 1.0,\n",
       " 'P(most|at)': 0.3333333333333333,\n",
       " 'P(a|most)': 1.0,\n",
       " 'P(few|a)': 0.3333333333333333,\n",
       " 'P(billion|few)': 1.0,\n",
       " 'P(words|billion)': 1.0,\n",
       " 'P(we|words)': 0.2,\n",
       " 'P(harnessing|been)': 0.3333333333333333,\n",
       " 'P(the|harnessing)': 1.0,\n",
       " 'P(vast|the)': 0.14285714285714285,\n",
       " 'P(power|vast)': 1.0,\n",
       " 'P(of|power)': 1.0,\n",
       " 'P(Google|of)': 0.1111111111111111,\n",
       " 'P(s|Google)': 0.5,\n",
       " 'P(datacenters|s)': 0.3333333333333333,\n",
       " 'P(and|datacenters)': 1.0,\n",
       " 'P(distributed|and)': 0.125,\n",
       " 'P(processing|distributed)': 1.0,\n",
       " 'P(infrastructure|processing)': 1.0,\n",
       " 'P(to|infrastructure)': 1.0,\n",
       " 'P(process|to)': 0.25,\n",
       " 'P(larger|process)': 1.0,\n",
       " 'P(and|larger)': 0.5,\n",
       " 'P(larger|and)': 0.125,\n",
       " 'P(training|larger)': 0.5,\n",
       " 'P(</s>|corpora)': 0.5,\n",
       " 'P(We|<s>)': 0.375,\n",
       " 'P(found|We)': 0.3333333333333333,\n",
       " 'P(that|found)': 1.0,\n",
       " 'P(there|that)': 0.25,\n",
       " 'P(s|there)': 1.0,\n",
       " 'P(no|s)': 0.3333333333333333,\n",
       " 'P(data|no)': 0.5,\n",
       " 'P(like|data)': 0.2,\n",
       " 'P(more|like)': 1.0,\n",
       " 'P(data|more)': 0.5,\n",
       " 'P(and|data)': 0.2,\n",
       " 'P(scaled|and)': 0.125,\n",
       " 'P(up|scaled)': 1.0,\n",
       " 'P(the|up)': 1.0,\n",
       " 'P(size|the)': 0.14285714285714285,\n",
       " 'P(of|size)': 1.0,\n",
       " 'P(our|of)': 0.1111111111111111,\n",
       " 'P(data|our)': 1.0,\n",
       " 'P(by|data)': 0.2,\n",
       " 'P(one|by)': 1.0,\n",
       " 'P(order|one)': 0.3333333333333333,\n",
       " 'P(of|order)': 1.0,\n",
       " 'P(magnitude|of)': 0.1111111111111111,\n",
       " 'P(and|magnitude)': 1.0,\n",
       " 'P(then|and)': 0.25,\n",
       " 'P(another|then)': 0.5,\n",
       " 'P(and|another)': 1.0,\n",
       " 'P(one|then)': 0.5,\n",
       " 'P(more|one)': 0.3333333333333333,\n",
       " 'P(resulting|more)': 0.5,\n",
       " 'P(in|resulting)': 1.0,\n",
       " 'P(a|in)': 0.5,\n",
       " 'P(training|a)': 0.3333333333333333,\n",
       " 'P(corpus|training)': 0.3333333333333333,\n",
       " 'P(of|corpus)': 1.0,\n",
       " 'P(one|of)': 0.1111111111111111,\n",
       " 'P(trillion|one)': 0.3333333333333333,\n",
       " 'P(words|trillion)': 1.0,\n",
       " 'P(from|words)': 0.2,\n",
       " 'P(public|from)': 0.3333333333333333,\n",
       " 'P(Web|public)': 1.0,\n",
       " 'P(pages|Web)': 1.0,\n",
       " 'P(</s>|pages)': 1.0,\n",
       " 'P(believe|We)': 0.3333333333333333,\n",
       " 'P(that|believe)': 1.0,\n",
       " 'P(the|that)': 0.25,\n",
       " 'P(entire|the)': 0.14285714285714285,\n",
       " 'P(research|entire)': 1.0,\n",
       " 'P(community|research)': 0.3333333333333333,\n",
       " 'P(can|community)': 1.0,\n",
       " 'P(benefit|can)': 1.0,\n",
       " 'P(from|benefit)': 1.0,\n",
       " 'P(access|from)': 0.3333333333333333,\n",
       " 'P(to|access)': 1.0,\n",
       " 'P(such|to)': 0.25,\n",
       " 'P(massive|such)': 0.3333333333333333,\n",
       " 'P(amounts|massive)': 1.0,\n",
       " 'P(of|amounts)': 1.0,\n",
       " 'P(data|of)': 0.1111111111111111,\n",
       " 'P(</s>|data)': 0.2,\n",
       " 'P(It|<s>)': 0.125,\n",
       " 'P(will|It)': 1.0,\n",
       " 'P(advance|will)': 0.3333333333333333,\n",
       " 'P(the|advance)': 1.0,\n",
       " 'P(state|the)': 0.14285714285714285,\n",
       " 'P(of|state)': 1.0,\n",
       " 'P(the|of)': 0.1111111111111111,\n",
       " 'P(art|the)': 0.14285714285714285,\n",
       " 'P(it|art)': 1.0,\n",
       " 'P(will|it)': 1.0,\n",
       " 'P(focus|will)': 0.3333333333333333,\n",
       " 'P(research|focus)': 1.0,\n",
       " 'P(in|research)': 0.3333333333333333,\n",
       " 'P(the|in)': 0.5,\n",
       " 'P(promising|the)': 0.14285714285714285,\n",
       " 'P(direction|promising)': 1.0,\n",
       " 'P(of|direction)': 1.0,\n",
       " 'P(large|of)': 0.1111111111111111,\n",
       " 'P(scale|large)': 0.5,\n",
       " 'P(data|scale)': 1.0,\n",
       " 'P(driven|data)': 0.2,\n",
       " 'P(approaches|driven)': 1.0,\n",
       " 'P(and|approaches)': 1.0,\n",
       " 'P(it|and)': 0.125,\n",
       " 'P(allow|will)': 0.3333333333333333,\n",
       " 'P(all|allow)': 1.0,\n",
       " 'P(research|all)': 0.5,\n",
       " 'P(groups|research)': 0.3333333333333333,\n",
       " 'P(no|groups)': 1.0,\n",
       " 'P(matter|no)': 0.5,\n",
       " 'P(how|matter)': 1.0,\n",
       " 'P(large|how)': 1.0,\n",
       " 'P(or|large)': 0.5,\n",
       " 'P(small|or)': 1.0,\n",
       " 'P(their|small)': 1.0,\n",
       " 'P(computing|their)': 1.0,\n",
       " 'P(resources|computing)': 1.0,\n",
       " 'P(to|resources)': 1.0,\n",
       " 'P(play|to)': 0.25,\n",
       " 'P(together|play)': 1.0,\n",
       " 'P(</s>|together)': 1.0,\n",
       " 'P(That|<s>)': 0.125,\n",
       " 'P(s|That)': 1.0,\n",
       " 'P(why|s)': 0.3333333333333333,\n",
       " 'P(we|why)': 1.0,\n",
       " 'P(decided|we)': 0.3333333333333333,\n",
       " 'P(to|decided)': 1.0,\n",
       " 'P(share|to)': 0.25,\n",
       " 'P(this|share)': 1.0,\n",
       " 'P(enormous|this)': 1.0,\n",
       " 'P(dataset|enormous)': 1.0,\n",
       " 'P(with|dataset)': 1.0,\n",
       " 'P(everyone|with)': 1.0,\n",
       " 'P(</s>|everyone)': 1.0,\n",
       " 'P(processed|We)': 0.3333333333333333,\n",
       " 'P(words|processed)': 1.0,\n",
       " 'P(of|words)': 0.2,\n",
       " 'P(running|of)': 0.1111111111111111,\n",
       " 'P(text|running)': 1.0,\n",
       " 'P(and|text)': 1.0,\n",
       " 'P(are|and)': 0.125,\n",
       " 'P(publishing|are)': 0.5,\n",
       " 'P(the|publishing)': 1.0,\n",
       " 'P(counts|the)': 0.14285714285714285,\n",
       " 'P(for|counts)': 1.0,\n",
       " 'P(all|for)': 0.5,\n",
       " 'P(five|all)': 0.5,\n",
       " 'P(word|five)': 1.0,\n",
       " 'P(sequences|word)': 0.5,\n",
       " 'P(that|sequences)': 1.0,\n",
       " 'P(appear|that)': 0.5,\n",
       " 'P(at|appear)': 0.5,\n",
       " 'P(least|at)': 0.3333333333333333,\n",
       " 'P(times|least)': 1.0,\n",
       " 'P(</s>|times)': 1.0,\n",
       " 'P(There|<s>)': 0.125,\n",
       " 'P(are|There)': 1.0,\n",
       " 'P(unique|are)': 0.5,\n",
       " 'P(words|unique)': 1.0,\n",
       " 'P(after|words)': 0.2,\n",
       " 'P(discarding|after)': 1.0,\n",
       " 'P(words|discarding)': 1.0,\n",
       " 'P(that|words)': 0.2,\n",
       " 'P(less|appear)': 0.5,\n",
       " 'P(than|less)': 1.0,\n",
       " 'P(times|than)': 1.0}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngram_probability(blines,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'P(Here|<s1> <s>)': 0.125,\n",
       " 'P(at|<s> Here)': 1.0,\n",
       " 'P(Google|Here at)': 1.0,\n",
       " 'P(Research|at Google)': 1.0,\n",
       " 'P(we|Google Research)': 1.0,\n",
       " 'P(have|Research we)': 1.0,\n",
       " 'P(been|we have)': 1.0,\n",
       " 'P(using|have been)': 0.5,\n",
       " 'P(word|been using)': 1.0,\n",
       " 'P(n|using word)': 1.0,\n",
       " 'P(gram|word n)': 1.0,\n",
       " 'P(models|n gram)': 1.0,\n",
       " 'P(for|gram models)': 1.0,\n",
       " 'P(a|models for)': 1.0,\n",
       " 'P(variety|for a)': 1.0,\n",
       " 'P(of|a variety)': 1.0,\n",
       " 'P(R|variety of)': 1.0,\n",
       " 'P(D|of R)': 1.0,\n",
       " 'P(projects|R D)': 1.0,\n",
       " 'P(such|D projects)': 1.0,\n",
       " 'P(as|projects such)': 1.0,\n",
       " 'P(statistical|such as)': 1.0,\n",
       " 'P(machine|as statistical)': 1.0,\n",
       " 'P(translation|statistical machine)': 1.0,\n",
       " 'P(speech|machine translation)': 1.0,\n",
       " 'P(recognition|translation speech)': 1.0,\n",
       " 'P(spelling|speech recognition)': 1.0,\n",
       " 'P(correction|recognition spelling)': 1.0,\n",
       " 'P(entity|spelling correction)': 1.0,\n",
       " 'P(detection|correction entity)': 1.0,\n",
       " 'P(information|entity detection)': 1.0,\n",
       " 'P(extraction|detection information)': 1.0,\n",
       " 'P(and|information extraction)': 1.0,\n",
       " 'P(others|extraction and)': 1.0,\n",
       " 'P(</s>|and others)': 1.0,\n",
       " 'P(</s1>|others </s>)': 1.0,\n",
       " 'P(<s1>|</s> </s1>)': 0.0,\n",
       " 'P(<s>|</s1> <s1>)': 0.0,\n",
       " 'P(While|<s1> <s>)': 0.125,\n",
       " 'P(such|<s> While)': 1.0,\n",
       " 'P(models|While such)': 1.0,\n",
       " 'P(have|such models)': 1.0,\n",
       " 'P(usually|models have)': 1.0,\n",
       " 'P(been|have usually)': 1.0,\n",
       " 'P(estimated|usually been)': 1.0,\n",
       " 'P(from|been estimated)': 1.0,\n",
       " 'P(training|estimated from)': 1.0,\n",
       " 'P(corpora|from training)': 1.0,\n",
       " 'P(containing|training corpora)': 0.5,\n",
       " 'P(at|corpora containing)': 1.0,\n",
       " 'P(most|containing at)': 1.0,\n",
       " 'P(a|at most)': 1.0,\n",
       " 'P(few|most a)': 1.0,\n",
       " 'P(billion|a few)': 1.0,\n",
       " 'P(words|few billion)': 1.0,\n",
       " 'P(we|billion words)': 1.0,\n",
       " 'P(have|words we)': 1.0,\n",
       " 'P(harnessing|have been)': 0.5,\n",
       " 'P(the|been harnessing)': 1.0,\n",
       " 'P(vast|harnessing the)': 1.0,\n",
       " 'P(power|the vast)': 1.0,\n",
       " 'P(of|vast power)': 1.0,\n",
       " 'P(Google|power of)': 1.0,\n",
       " 'P(s|of Google)': 1.0,\n",
       " 'P(datacenters|Google s)': 1.0,\n",
       " 'P(and|s datacenters)': 1.0,\n",
       " 'P(distributed|datacenters and)': 1.0,\n",
       " 'P(processing|and distributed)': 1.0,\n",
       " 'P(infrastructure|distributed processing)': 1.0,\n",
       " 'P(to|processing infrastructure)': 1.0,\n",
       " 'P(process|infrastructure to)': 1.0,\n",
       " 'P(larger|to process)': 1.0,\n",
       " 'P(and|process larger)': 1.0,\n",
       " 'P(larger|larger and)': 1.0,\n",
       " 'P(training|and larger)': 1.0,\n",
       " 'P(corpora|larger training)': 1.0,\n",
       " 'P(</s>|training corpora)': 0.5,\n",
       " 'P(</s1>|corpora </s>)': 1.0,\n",
       " 'P(We|<s1> <s>)': 0.375,\n",
       " 'P(found|<s> We)': 0.3333333333333333,\n",
       " 'P(that|We found)': 1.0,\n",
       " 'P(there|found that)': 1.0,\n",
       " 'P(s|that there)': 1.0,\n",
       " 'P(no|there s)': 1.0,\n",
       " 'P(data|s no)': 1.0,\n",
       " 'P(like|no data)': 1.0,\n",
       " 'P(more|data like)': 1.0,\n",
       " 'P(data|like more)': 1.0,\n",
       " 'P(and|more data)': 1.0,\n",
       " 'P(scaled|data and)': 1.0,\n",
       " 'P(up|and scaled)': 1.0,\n",
       " 'P(the|scaled up)': 1.0,\n",
       " 'P(size|up the)': 1.0,\n",
       " 'P(of|the size)': 1.0,\n",
       " 'P(our|size of)': 1.0,\n",
       " 'P(data|of our)': 1.0,\n",
       " 'P(by|our data)': 1.0,\n",
       " 'P(one|data by)': 1.0,\n",
       " 'P(order|by one)': 1.0,\n",
       " 'P(of|one order)': 1.0,\n",
       " 'P(magnitude|order of)': 1.0,\n",
       " 'P(and|of magnitude)': 1.0,\n",
       " 'P(then|magnitude and)': 1.0,\n",
       " 'P(another|and then)': 0.5,\n",
       " 'P(and|then another)': 1.0,\n",
       " 'P(then|another and)': 1.0,\n",
       " 'P(one|and then)': 0.5,\n",
       " 'P(more|then one)': 1.0,\n",
       " 'P(resulting|one more)': 1.0,\n",
       " 'P(in|more resulting)': 1.0,\n",
       " 'P(a|resulting in)': 1.0,\n",
       " 'P(training|in a)': 1.0,\n",
       " 'P(corpus|a training)': 1.0,\n",
       " 'P(of|training corpus)': 1.0,\n",
       " 'P(one|corpus of)': 1.0,\n",
       " 'P(trillion|of one)': 1.0,\n",
       " 'P(words|one trillion)': 1.0,\n",
       " 'P(from|trillion words)': 1.0,\n",
       " 'P(public|words from)': 1.0,\n",
       " 'P(Web|from public)': 1.0,\n",
       " 'P(pages|public Web)': 1.0,\n",
       " 'P(</s>|Web pages)': 1.0,\n",
       " 'P(</s1>|pages </s>)': 1.0,\n",
       " 'P(believe|<s> We)': 0.3333333333333333,\n",
       " 'P(that|We believe)': 1.0,\n",
       " 'P(the|believe that)': 1.0,\n",
       " 'P(entire|that the)': 1.0,\n",
       " 'P(research|the entire)': 1.0,\n",
       " 'P(community|entire research)': 1.0,\n",
       " 'P(can|research community)': 1.0,\n",
       " 'P(benefit|community can)': 1.0,\n",
       " 'P(from|can benefit)': 1.0,\n",
       " 'P(access|benefit from)': 1.0,\n",
       " 'P(to|from access)': 1.0,\n",
       " 'P(such|access to)': 1.0,\n",
       " 'P(massive|to such)': 1.0,\n",
       " 'P(amounts|such massive)': 1.0,\n",
       " 'P(of|massive amounts)': 1.0,\n",
       " 'P(data|amounts of)': 1.0,\n",
       " 'P(</s>|of data)': 1.0,\n",
       " 'P(</s1>|data </s>)': 1.0,\n",
       " 'P(It|<s1> <s>)': 0.125,\n",
       " 'P(will|<s> It)': 1.0,\n",
       " 'P(advance|It will)': 1.0,\n",
       " 'P(the|will advance)': 1.0,\n",
       " 'P(state|advance the)': 1.0,\n",
       " 'P(of|the state)': 1.0,\n",
       " 'P(the|state of)': 1.0,\n",
       " 'P(art|of the)': 1.0,\n",
       " 'P(it|the art)': 1.0,\n",
       " 'P(will|art it)': 1.0,\n",
       " 'P(focus|it will)': 0.5,\n",
       " 'P(research|will focus)': 1.0,\n",
       " 'P(in|focus research)': 1.0,\n",
       " 'P(the|research in)': 1.0,\n",
       " 'P(promising|in the)': 1.0,\n",
       " 'P(direction|the promising)': 1.0,\n",
       " 'P(of|promising direction)': 1.0,\n",
       " 'P(large|direction of)': 1.0,\n",
       " 'P(scale|of large)': 1.0,\n",
       " 'P(data|large scale)': 1.0,\n",
       " 'P(driven|scale data)': 1.0,\n",
       " 'P(approaches|data driven)': 1.0,\n",
       " 'P(and|driven approaches)': 1.0,\n",
       " 'P(it|approaches and)': 1.0,\n",
       " 'P(will|and it)': 1.0,\n",
       " 'P(allow|it will)': 0.5,\n",
       " 'P(all|will allow)': 1.0,\n",
       " 'P(research|allow all)': 1.0,\n",
       " 'P(groups|all research)': 1.0,\n",
       " 'P(no|research groups)': 1.0,\n",
       " 'P(matter|groups no)': 1.0,\n",
       " 'P(how|no matter)': 1.0,\n",
       " 'P(large|matter how)': 1.0,\n",
       " 'P(or|how large)': 1.0,\n",
       " 'P(small|large or)': 1.0,\n",
       " 'P(their|or small)': 1.0,\n",
       " 'P(computing|small their)': 1.0,\n",
       " 'P(resources|their computing)': 1.0,\n",
       " 'P(to|computing resources)': 1.0,\n",
       " 'P(play|resources to)': 1.0,\n",
       " 'P(together|to play)': 1.0,\n",
       " 'P(</s>|play together)': 1.0,\n",
       " 'P(</s1>|together </s>)': 1.0,\n",
       " 'P(That|<s1> <s>)': 0.125,\n",
       " 'P(s|<s> That)': 1.0,\n",
       " 'P(why|That s)': 1.0,\n",
       " 'P(we|s why)': 1.0,\n",
       " 'P(decided|why we)': 1.0,\n",
       " 'P(to|we decided)': 1.0,\n",
       " 'P(share|decided to)': 1.0,\n",
       " 'P(this|to share)': 1.0,\n",
       " 'P(enormous|share this)': 1.0,\n",
       " 'P(dataset|this enormous)': 1.0,\n",
       " 'P(with|enormous dataset)': 1.0,\n",
       " 'P(everyone|dataset with)': 1.0,\n",
       " 'P(</s>|with everyone)': 1.0,\n",
       " 'P(</s1>|everyone </s>)': 1.0,\n",
       " 'P(processed|<s> We)': 0.3333333333333333,\n",
       " 'P(words|We processed)': 1.0,\n",
       " 'P(of|processed words)': 1.0,\n",
       " 'P(running|words of)': 1.0,\n",
       " 'P(text|of running)': 1.0,\n",
       " 'P(and|running text)': 1.0,\n",
       " 'P(are|text and)': 1.0,\n",
       " 'P(publishing|and are)': 1.0,\n",
       " 'P(the|are publishing)': 1.0,\n",
       " 'P(counts|publishing the)': 1.0,\n",
       " 'P(for|the counts)': 1.0,\n",
       " 'P(all|counts for)': 1.0,\n",
       " 'P(five|for all)': 1.0,\n",
       " 'P(word|all five)': 1.0,\n",
       " 'P(sequences|five word)': 1.0,\n",
       " 'P(that|word sequences)': 1.0,\n",
       " 'P(appear|sequences that)': 1.0,\n",
       " 'P(at|that appear)': 0.5,\n",
       " 'P(least|appear at)': 1.0,\n",
       " 'P(times|at least)': 1.0,\n",
       " 'P(</s>|least times)': 1.0,\n",
       " 'P(</s1>|times </s>)': 1.0,\n",
       " 'P(There|<s1> <s>)': 0.125,\n",
       " 'P(are|<s> There)': 1.0,\n",
       " 'P(unique|There are)': 1.0,\n",
       " 'P(words|are unique)': 1.0,\n",
       " 'P(after|unique words)': 1.0,\n",
       " 'P(discarding|words after)': 1.0,\n",
       " 'P(words|after discarding)': 1.0,\n",
       " 'P(that|discarding words)': 1.0,\n",
       " 'P(appear|words that)': 1.0,\n",
       " 'P(less|that appear)': 0.5,\n",
       " 'P(than|appear less)': 1.0,\n",
       " 'P(times|less than)': 1.0,\n",
       " 'P(</s>|than times)': 1.0}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngram_probability(tlines,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'P(Here|<s2> <s1> <s>)': 0.125,\n",
       " 'P(at|<s1> <s> Here)': 1.0,\n",
       " 'P(Google|<s> Here at)': 1.0,\n",
       " 'P(Research|Here at Google)': 1.0,\n",
       " 'P(we|at Google Research)': 1.0,\n",
       " 'P(have|Google Research we)': 1.0,\n",
       " 'P(been|Research we have)': 1.0,\n",
       " 'P(using|we have been)': 0.5,\n",
       " 'P(word|have been using)': 1.0,\n",
       " 'P(n|been using word)': 1.0,\n",
       " 'P(gram|using word n)': 1.0,\n",
       " 'P(models|word n gram)': 1.0,\n",
       " 'P(for|n gram models)': 1.0,\n",
       " 'P(a|gram models for)': 1.0,\n",
       " 'P(variety|models for a)': 1.0,\n",
       " 'P(of|for a variety)': 1.0,\n",
       " 'P(R|a variety of)': 1.0,\n",
       " 'P(D|variety of R)': 1.0,\n",
       " 'P(projects|of R D)': 1.0,\n",
       " 'P(such|R D projects)': 1.0,\n",
       " 'P(as|D projects such)': 1.0,\n",
       " 'P(statistical|projects such as)': 1.0,\n",
       " 'P(machine|such as statistical)': 1.0,\n",
       " 'P(translation|as statistical machine)': 1.0,\n",
       " 'P(speech|statistical machine translation)': 1.0,\n",
       " 'P(recognition|machine translation speech)': 1.0,\n",
       " 'P(spelling|translation speech recognition)': 1.0,\n",
       " 'P(correction|speech recognition spelling)': 1.0,\n",
       " 'P(entity|recognition spelling correction)': 1.0,\n",
       " 'P(detection|spelling correction entity)': 1.0,\n",
       " 'P(information|correction entity detection)': 1.0,\n",
       " 'P(extraction|entity detection information)': 1.0,\n",
       " 'P(and|detection information extraction)': 1.0,\n",
       " 'P(others|information extraction and)': 1.0,\n",
       " 'P(</s>|extraction and others)': 1.0,\n",
       " 'P(</s1>|and others </s>)': 1.0,\n",
       " 'P(</s2>|others </s> </s1>)': 1.0,\n",
       " 'P(<s2>|</s> </s1> </s2>)': 0.0,\n",
       " 'P(<s1>|</s1> </s2> <s2>)': 0.0,\n",
       " 'P(<s>|</s2> <s2> <s1>)': 0.0,\n",
       " 'P(While|<s2> <s1> <s>)': 0.125,\n",
       " 'P(such|<s1> <s> While)': 1.0,\n",
       " 'P(models|<s> While such)': 1.0,\n",
       " 'P(have|While such models)': 1.0,\n",
       " 'P(usually|such models have)': 1.0,\n",
       " 'P(been|models have usually)': 1.0,\n",
       " 'P(estimated|have usually been)': 1.0,\n",
       " 'P(from|usually been estimated)': 1.0,\n",
       " 'P(training|been estimated from)': 1.0,\n",
       " 'P(corpora|estimated from training)': 1.0,\n",
       " 'P(containing|from training corpora)': 1.0,\n",
       " 'P(at|training corpora containing)': 1.0,\n",
       " 'P(most|corpora containing at)': 1.0,\n",
       " 'P(a|containing at most)': 1.0,\n",
       " 'P(few|at most a)': 1.0,\n",
       " 'P(billion|most a few)': 1.0,\n",
       " 'P(words|a few billion)': 1.0,\n",
       " 'P(we|few billion words)': 1.0,\n",
       " 'P(have|billion words we)': 1.0,\n",
       " 'P(been|words we have)': 1.0,\n",
       " 'P(harnessing|we have been)': 0.5,\n",
       " 'P(the|have been harnessing)': 1.0,\n",
       " 'P(vast|been harnessing the)': 1.0,\n",
       " 'P(power|harnessing the vast)': 1.0,\n",
       " 'P(of|the vast power)': 1.0,\n",
       " 'P(Google|vast power of)': 1.0,\n",
       " 'P(s|power of Google)': 1.0,\n",
       " 'P(datacenters|of Google s)': 1.0,\n",
       " 'P(and|Google s datacenters)': 1.0,\n",
       " 'P(distributed|s datacenters and)': 1.0,\n",
       " 'P(processing|datacenters and distributed)': 1.0,\n",
       " 'P(infrastructure|and distributed processing)': 1.0,\n",
       " 'P(to|distributed processing infrastructure)': 1.0,\n",
       " 'P(process|processing infrastructure to)': 1.0,\n",
       " 'P(larger|infrastructure to process)': 1.0,\n",
       " 'P(and|to process larger)': 1.0,\n",
       " 'P(larger|process larger and)': 1.0,\n",
       " 'P(training|larger and larger)': 1.0,\n",
       " 'P(corpora|and larger training)': 1.0,\n",
       " 'P(</s>|larger training corpora)': 1.0,\n",
       " 'P(</s1>|training corpora </s>)': 1.0,\n",
       " 'P(</s2>|corpora </s> </s1>)': 1.0,\n",
       " 'P(We|<s2> <s1> <s>)': 0.375,\n",
       " 'P(found|<s1> <s> We)': 0.3333333333333333,\n",
       " 'P(that|<s> We found)': 1.0,\n",
       " 'P(there|We found that)': 1.0,\n",
       " 'P(s|found that there)': 1.0,\n",
       " 'P(no|that there s)': 1.0,\n",
       " 'P(data|there s no)': 1.0,\n",
       " 'P(like|s no data)': 1.0,\n",
       " 'P(more|no data like)': 1.0,\n",
       " 'P(data|data like more)': 1.0,\n",
       " 'P(and|like more data)': 1.0,\n",
       " 'P(scaled|more data and)': 1.0,\n",
       " 'P(up|data and scaled)': 1.0,\n",
       " 'P(the|and scaled up)': 1.0,\n",
       " 'P(size|scaled up the)': 1.0,\n",
       " 'P(of|up the size)': 1.0,\n",
       " 'P(our|the size of)': 1.0,\n",
       " 'P(data|size of our)': 1.0,\n",
       " 'P(by|of our data)': 1.0,\n",
       " 'P(one|our data by)': 1.0,\n",
       " 'P(order|data by one)': 1.0,\n",
       " 'P(of|by one order)': 1.0,\n",
       " 'P(magnitude|one order of)': 1.0,\n",
       " 'P(and|order of magnitude)': 1.0,\n",
       " 'P(then|of magnitude and)': 1.0,\n",
       " 'P(another|magnitude and then)': 1.0,\n",
       " 'P(and|and then another)': 1.0,\n",
       " 'P(then|then another and)': 1.0,\n",
       " 'P(one|another and then)': 1.0,\n",
       " 'P(more|and then one)': 1.0,\n",
       " 'P(resulting|then one more)': 1.0,\n",
       " 'P(in|one more resulting)': 1.0,\n",
       " 'P(a|more resulting in)': 1.0,\n",
       " 'P(training|resulting in a)': 1.0,\n",
       " 'P(corpus|in a training)': 1.0,\n",
       " 'P(of|a training corpus)': 1.0,\n",
       " 'P(one|training corpus of)': 1.0,\n",
       " 'P(trillion|corpus of one)': 1.0,\n",
       " 'P(words|of one trillion)': 1.0,\n",
       " 'P(from|one trillion words)': 1.0,\n",
       " 'P(public|trillion words from)': 1.0,\n",
       " 'P(Web|words from public)': 1.0,\n",
       " 'P(pages|from public Web)': 1.0,\n",
       " 'P(</s>|public Web pages)': 1.0,\n",
       " 'P(</s1>|Web pages </s>)': 1.0,\n",
       " 'P(</s2>|pages </s> </s1>)': 1.0,\n",
       " 'P(believe|<s1> <s> We)': 0.3333333333333333,\n",
       " 'P(that|<s> We believe)': 1.0,\n",
       " 'P(the|We believe that)': 1.0,\n",
       " 'P(entire|believe that the)': 1.0,\n",
       " 'P(research|that the entire)': 1.0,\n",
       " 'P(community|the entire research)': 1.0,\n",
       " 'P(can|entire research community)': 1.0,\n",
       " 'P(benefit|research community can)': 1.0,\n",
       " 'P(from|community can benefit)': 1.0,\n",
       " 'P(access|can benefit from)': 1.0,\n",
       " 'P(to|benefit from access)': 1.0,\n",
       " 'P(such|from access to)': 1.0,\n",
       " 'P(massive|access to such)': 1.0,\n",
       " 'P(amounts|to such massive)': 1.0,\n",
       " 'P(of|such massive amounts)': 1.0,\n",
       " 'P(data|massive amounts of)': 1.0,\n",
       " 'P(</s>|amounts of data)': 1.0,\n",
       " 'P(</s1>|of data </s>)': 1.0,\n",
       " 'P(</s2>|data </s> </s1>)': 1.0,\n",
       " 'P(It|<s2> <s1> <s>)': 0.125,\n",
       " 'P(will|<s1> <s> It)': 1.0,\n",
       " 'P(advance|<s> It will)': 1.0,\n",
       " 'P(the|It will advance)': 1.0,\n",
       " 'P(state|will advance the)': 1.0,\n",
       " 'P(of|advance the state)': 1.0,\n",
       " 'P(the|the state of)': 1.0,\n",
       " 'P(art|state of the)': 1.0,\n",
       " 'P(it|of the art)': 1.0,\n",
       " 'P(will|the art it)': 1.0,\n",
       " 'P(focus|art it will)': 1.0,\n",
       " 'P(research|it will focus)': 1.0,\n",
       " 'P(in|will focus research)': 1.0,\n",
       " 'P(the|focus research in)': 1.0,\n",
       " 'P(promising|research in the)': 1.0,\n",
       " 'P(direction|in the promising)': 1.0,\n",
       " 'P(of|the promising direction)': 1.0,\n",
       " 'P(large|promising direction of)': 1.0,\n",
       " 'P(scale|direction of large)': 1.0,\n",
       " 'P(data|of large scale)': 1.0,\n",
       " 'P(driven|large scale data)': 1.0,\n",
       " 'P(approaches|scale data driven)': 1.0,\n",
       " 'P(and|data driven approaches)': 1.0,\n",
       " 'P(it|driven approaches and)': 1.0,\n",
       " 'P(will|approaches and it)': 1.0,\n",
       " 'P(allow|and it will)': 1.0,\n",
       " 'P(all|it will allow)': 1.0,\n",
       " 'P(research|will allow all)': 1.0,\n",
       " 'P(groups|allow all research)': 1.0,\n",
       " 'P(no|all research groups)': 1.0,\n",
       " 'P(matter|research groups no)': 1.0,\n",
       " 'P(how|groups no matter)': 1.0,\n",
       " 'P(large|no matter how)': 1.0,\n",
       " 'P(or|matter how large)': 1.0,\n",
       " 'P(small|how large or)': 1.0,\n",
       " 'P(their|large or small)': 1.0,\n",
       " 'P(computing|or small their)': 1.0,\n",
       " 'P(resources|small their computing)': 1.0,\n",
       " 'P(to|their computing resources)': 1.0,\n",
       " 'P(play|computing resources to)': 1.0,\n",
       " 'P(together|resources to play)': 1.0,\n",
       " 'P(</s>|to play together)': 1.0,\n",
       " 'P(</s1>|play together </s>)': 1.0,\n",
       " 'P(</s2>|together </s> </s1>)': 1.0,\n",
       " 'P(That|<s2> <s1> <s>)': 0.125,\n",
       " 'P(s|<s1> <s> That)': 1.0,\n",
       " 'P(why|<s> That s)': 1.0,\n",
       " 'P(we|That s why)': 1.0,\n",
       " 'P(decided|s why we)': 1.0,\n",
       " 'P(to|why we decided)': 1.0,\n",
       " 'P(share|we decided to)': 1.0,\n",
       " 'P(this|decided to share)': 1.0,\n",
       " 'P(enormous|to share this)': 1.0,\n",
       " 'P(dataset|share this enormous)': 1.0,\n",
       " 'P(with|this enormous dataset)': 1.0,\n",
       " 'P(everyone|enormous dataset with)': 1.0,\n",
       " 'P(</s>|dataset with everyone)': 1.0,\n",
       " 'P(</s1>|with everyone </s>)': 1.0,\n",
       " 'P(</s2>|everyone </s> </s1>)': 1.0,\n",
       " 'P(processed|<s1> <s> We)': 0.3333333333333333,\n",
       " 'P(words|<s> We processed)': 1.0,\n",
       " 'P(of|We processed words)': 1.0,\n",
       " 'P(running|processed words of)': 1.0,\n",
       " 'P(text|words of running)': 1.0,\n",
       " 'P(and|of running text)': 1.0,\n",
       " 'P(are|running text and)': 1.0,\n",
       " 'P(publishing|text and are)': 1.0,\n",
       " 'P(the|and are publishing)': 1.0,\n",
       " 'P(counts|are publishing the)': 1.0,\n",
       " 'P(for|publishing the counts)': 1.0,\n",
       " 'P(all|the counts for)': 1.0,\n",
       " 'P(five|counts for all)': 1.0,\n",
       " 'P(word|for all five)': 1.0,\n",
       " 'P(sequences|all five word)': 1.0,\n",
       " 'P(that|five word sequences)': 1.0,\n",
       " 'P(appear|word sequences that)': 1.0,\n",
       " 'P(at|sequences that appear)': 1.0,\n",
       " 'P(least|that appear at)': 1.0,\n",
       " 'P(times|appear at least)': 1.0,\n",
       " 'P(</s>|at least times)': 1.0,\n",
       " 'P(</s1>|least times </s>)': 1.0,\n",
       " 'P(</s2>|times </s> </s1>)': 1.0,\n",
       " 'P(There|<s2> <s1> <s>)': 0.125,\n",
       " 'P(are|<s1> <s> There)': 1.0,\n",
       " 'P(unique|<s> There are)': 1.0,\n",
       " 'P(words|There are unique)': 1.0,\n",
       " 'P(after|are unique words)': 1.0,\n",
       " 'P(discarding|unique words after)': 1.0,\n",
       " 'P(words|words after discarding)': 1.0,\n",
       " 'P(that|after discarding words)': 1.0,\n",
       " 'P(appear|discarding words that)': 1.0,\n",
       " 'P(less|words that appear)': 1.0,\n",
       " 'P(than|that appear less)': 1.0,\n",
       " 'P(times|appear less than)': 1.0,\n",
       " 'P(</s>|less than times)': 1.0,\n",
       " 'P(</s1>|than times </s>)': 1.0}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngram_probability(flines,4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
